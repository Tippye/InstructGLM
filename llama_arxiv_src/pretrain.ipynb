{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-02T06:43:18.261639Z",
     "start_time": "2024-07-02T06:43:18.258747Z"
    }
   },
   "source": [
    "import collections\n",
    "import pickle\n",
    "from dis import dis\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import shutil\n",
    "from packaging import version\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3,4,5,6,7\"\n",
    "from param import parse_args\n",
    "from pretrain_data import get_loader, load_pickle\n",
    "from utils import LossMeter\n",
    "from dist_utils import reduce_dict, new_reduce_dict\n",
    "from transformers import LlamaTokenizerFast\n",
    "from flan_arxiv_src.param import Config"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:45:36.273957Z",
     "start_time": "2024-07-02T06:45:36.269848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = {\n",
    "    'distributed': False,\n",
    "    'multiGPU': False,\n",
    "    'seed': 42,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'train': 'Arxiv',\n",
    "    'valid': 'Arxiv',\n",
    "    'batch_size': 4,\n",
    "    'optim': 'adamw',\n",
    "    'warmup_ratio': 0.05,\n",
    "    'num_workers': 4,\n",
    "    'clip_grad_norm': 1.0,\n",
    "    'losses': 'link,classification',\n",
    "    'backbone': '/Volumes/share/Papers/Language is All a Graph Needs/InstructGLM/7B',\n",
    "    'output': 'snap/arxiv-7b',\n",
    "    'epoch': 2,\n",
    "    'weight_decay': 0,\n",
    "    'max_text_length': 2048,\n",
    "    'gen_max_length': 64,\n",
    "    'lr': 8*1e-5,\n",
    "    'comment': '',\n",
    "    'mode': 'train',\n",
    "    'data_dir': '/Volumes/share/Papers/Language is All a Graph Needs/InstructGLM/Arxiv/my_data',\n",
    "    'gpu': torch.device('mps'),\n",
    "    'dropout': 0.1,\n",
    "}"
   ],
   "id": "a3cda052d99d6836",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:45:36.414486Z",
     "start_time": "2024-07-02T06:45:36.411359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LOSSES_NAME = [f'{name}_loss' for name in args['losses'].split(',')]\n",
    "LOSSES_NAME.append('total_loss')\n",
    "args['LOSSES_NAME'] = LOSSES_NAME\n",
    "\n",
    "comments = []\n",
    "dsets = []\n",
    "if 'Arxiv' in args['train']:\n",
    "    dsets.append('Arxiv')\n",
    "comments.append(''.join(dsets))\n",
    "\n",
    "comments.append(args['backbone'])\n",
    "comments.append(''.join(args['losses'].split(',')))\n",
    "if args['comment'] != '':\n",
    "    comments.append(args['comment'])\n",
    "    \n",
    "args = Config(**args)"
   ],
   "id": "1005526a857bc1cc",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:28:33.604523Z",
     "start_time": "2024-07-02T06:28:33.602476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_task_list = {\n",
    "                'classification': [['6-6-6-7'], ['2-1-1-2', '2-3-1-2'], ['2-1-2-2', '2-1-2-4', '2-3-2-2', '2-3-2-4'],\n",
    "                                   ['2-1-3-2', '2-1-3-4', '2-3-3-2', '2-3-3-4']],\n",
    "                # 'link':[['1-1-3-1']]\n",
    "                'link': [['1-1-1-1', '1-3-1-1'], ['1-1-2-1', '1-1-2-3', '1-3-2-1', '1-3-2-3'],\n",
    "                         ['1-1-3-1', '1-1-3-3', '1-3-3-1', '1-3-3-3']]\n",
    "            }\n",
    "train_sample_numbers = {}"
   ],
   "id": "4ed77d24118d055",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:28:34.003533Z",
     "start_time": "2024-07-02T06:28:33.608500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = LlamaTokenizerFast.from_pretrained(args.backbone)\n",
    "tokenizer.pad_token=tokenizer.unk_token\n",
    "special={'additional_special_tokens': ['<extra_id_0>']}\n",
    "tokenizer.add_special_tokens(special)"
   ],
   "id": "6f32c381b8d280de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:32:02.202302Z",
     "start_time": "2024-07-02T06:28:34.004458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from all_graph_templates import all_tasks as task_templates\n",
    "from arxiv import Arxiv_Dataset\n",
    "\n",
    "dataset = Arxiv_Dataset(\n",
    "    task_templates,\n",
    "    task_list=train_task_list,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    sample_numbers=train_sample_numbers,\n",
    "    mode=args.mode,\n",
    "    split=args.train,\n",
    "    rating_augment=False,\n",
    "    data_dir=args.data_dir,\n",
    ")"
   ],
   "id": "780c619a1ca27da9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['Arxiv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 726286.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_datum_info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181882/181882 [00:00<00:00, 3427466.70it/s]\n",
      "100%|██████████| 90941/90941 [00:00<00:00, 3644020.48it/s]\n",
      "100%|██████████| 90941/90941 [00:00<00:00, 3554739.38it/s]\n",
      "100%|██████████| 90941/90941 [00:00<00:00, 3701914.85it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:32:02.206949Z",
     "start_time": "2024-07-02T06:32:02.203928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "sampler = None\n",
    "train_loader = DataLoader(\n",
    "            dataset, batch_size=args.batch_size, shuffle=(sampler is None),\n",
    "            num_workers=args.num_workers, pin_memory=True, sampler=sampler,\n",
    "            collate_fn=dataset.collate_fn, drop_last=False)"
   ],
   "id": "d591293f0e54480a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:32:02.210026Z",
     "start_time": "2024-07-02T06:32:02.207918Z"
    }
   },
   "cell_type": "code",
   "source": "print('Length of train dataset:', len(train_loader.dataset))",
   "id": "179d4cc5a9f6265e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 962734\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:32:02.893246Z",
     "start_time": "2024-07-02T06:32:02.211005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trainer_base import TrainerBase\n",
    "from pretrain_model import InstructGLM\n",
    "from peft import (  # LoRA Setting\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "# _use_native_amp = False\n",
    "_use_apex = False\n",
    "\n",
    "_use_native_amp = True\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "class FP(\n",
    "    nn.Module):  # first_model, i.e. simple MLP for dimension transformation of OGB/ GIANT node embedding + freezed Llama-7b word embeddings.\n",
    "    def __init__(self, llama_embed, real):\n",
    "        super(FP, self).__init__()\n",
    "        self.trans_2 = nn.Linear(512, 4096, bias=False)\n",
    "        self.trans_1 = nn.Linear(768, 512, bias=False)\n",
    "        self.rac = nn.ELU()\n",
    "        self.sln = nn.LayerNorm(512)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.LayerNorm):\n",
    "                m.weight.data.fill_(1.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(mean=0.0, std=1.0)\n",
    "\n",
    "        self.embed_tokens = llama_embed  # freezed Llama-7b word embeddings.\n",
    "\n",
    "        self.real = real\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        transfered = self.trans_2(self.rac(self.sln(self.trans_1(self.real))))\n",
    "\n",
    "        inputs_embeds = transfered[input_ids] + self.embed_tokens[input_ids]  ### embedding step - add HERE ###\n",
    "        # transfered contains shaped OGB/ Giant node embedding, while self.embed_tokens contains natural language word embedding.\n",
    "\n",
    "        return inputs_embeds"
   ],
   "id": "d3825d7991745397",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tippy/Desktop/项目/InstructGLM/venv/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/tippy/Desktop/项目/InstructGLM/venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /Users/tippy/Desktop/项目/InstructGLM/venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/tippy/Desktop/项目/InstructGLM/venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/tippy/Desktop/项目/InstructGLM/venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/tippy/Desktop/项目/InstructGLM/venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/tippy/Desktop/项目/InstructGLM/venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:44:51.684176Z",
     "start_time": "2024-07-02T06:44:51.605130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "from trainer_base import TrainerBase\n",
    "\n",
    "# The Trainer inherits TrainerBase in trainer_base.py\n",
    "class Trainer(TrainerBase):\n",
    "    def __init__(self, args, train_loader=None, val_loader=None, test_loader=None, train=True, val_list=None):\n",
    "        super().__init__(\n",
    "            args,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            train=train)\n",
    "\n",
    "        model_class = InstructGLM\n",
    "        self.m_class = InstructGLM\n",
    "\n",
    "        config = self.create_config()\n",
    "        self.m_config = config\n",
    "        self.tokenizer = self.create_tokenizer()\n",
    "\n",
    "        re_start = 0  # main model restart way control\n",
    "        if train:\n",
    "\n",
    "            self.model = self.create_model(model_class, config)\n",
    "            self.model.tokenizer = self.tokenizer\n",
    "\n",
    "            self.model = prepare_model_for_int8_training(self.model)\n",
    "\n",
    "            # Form peft-model.\n",
    "            lora_r = 16\n",
    "            lora_alpha = 16\n",
    "            lora_target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'lm_head']  # Select LoRA tuning modules.\n",
    "            lora_dropout = 0.05\n",
    "\n",
    "            LORA_config = LoraConfig(r=lora_r, lora_alpha=lora_alpha, target_modules=lora_target_modules,\n",
    "                                     lora_dropout=lora_dropout, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "            if re_start != 2:\n",
    "                self.model = get_peft_model(self.model, LORA_config)\n",
    "\n",
    "            if self.verbose and re_start != 2:\n",
    "                print()\n",
    "                self.model.print_trainable_parameters()\n",
    "                print()\n",
    "            dist.barrier()\n",
    "\n",
    "            if re_start == 1:\n",
    "                print('Main model re-starting')\n",
    "                doc_prefix = './your_folder_name_with_pickle/'\n",
    "                for gg in range(32):\n",
    "                    self.model.base_model.model.model.layers[\n",
    "                        gg].self_attn.q_proj.lora_A.default.weight.data = load_pickle(\n",
    "                        doc_prefix + \"Gqa_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                    self.model.base_model.model.model.layers[\n",
    "                        gg].self_attn.k_proj.lora_A.default.weight.data = load_pickle(\n",
    "                        doc_prefix + \"Gka_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                    self.model.base_model.model.model.layers[\n",
    "                        gg].self_attn.v_proj.lora_A.default.weight.data = load_pickle(\n",
    "                        doc_prefix + \"Gva_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                    self.model.base_model.model.model.layers[\n",
    "                        gg].self_attn.o_proj.lora_A.default.weight.data = load_pickle(\n",
    "                        doc_prefix + \"Goa_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                    self.model.base_model.model.model.layers[\n",
    "                        gg].self_attn.q_proj.lora_B.default.weight.data = load_pickle(\n",
    "                        doc_prefix + \"Gqb_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                    self.model.base_model.model.model.layers[\n",
    "                        gg].self_attn.k_proj.lora_B.default.weight.data = load_pickle(\n",
    "                        doc_prefix + \"Gkb_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                    self.model.base_model.model.model.layers[\n",
    "                        gg].self_attn.v_proj.lora_B.default.weight.data = load_pickle(\n",
    "                        doc_prefix + \"Gvb_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                    self.model.base_model.model.model.layers[\n",
    "                        gg].self_attn.o_proj.lora_B.default.weight.data = load_pickle(\n",
    "                        doc_prefix + \"Gob_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                self.model.base_model.model.lm_head.lora_A.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Glm_a_{}.pkl\".format(self.args.lr)).data.to(args.gpu)\n",
    "                self.model.base_model.model.lm_head.lora_B.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Glm_b_{}.pkl\".format(self.args.lr)).data.to(args.gpu)\n",
    "                print('Main model loaded.')\n",
    "\n",
    "                if self.verbose:\n",
    "                    self.model.save_pretrained(\"G_restart\")  # Another way for checkpoint saving\n",
    "                dist.barrier()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if 'lora' in n:\n",
    "                        p.requires_grad_()\n",
    "                if self.verbose:\n",
    "                    print()\n",
    "                    self.model.print_trainable_parameters()\n",
    "                    print()\n",
    "\n",
    "            if re_start == 2:  # recommended\n",
    "                from peft import PeftModel, PeftConfig\n",
    "                peft_model_id = './your_folder_name_with_bin'\n",
    "                print('now we are loading peft model')\n",
    "\n",
    "                self.model = PeftModel.from_pretrained(self.model, peft_model_id)  # Another way for loading checkpoint\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if 'lora' in n:\n",
    "                        p.requires_grad_()\n",
    "                if self.verbose:\n",
    "                    print()\n",
    "                    self.model.print_trainable_parameters()\n",
    "                    print()\n",
    "\n",
    "            self.model = self.model.to(args.gpu)\n",
    "        else:  # Inference\n",
    "            self.model = self.create_model(model_class, config)\n",
    "            self.model.tokenizer = self.tokenizer\n",
    "\n",
    "            for name, param in self.model.named_parameters():\n",
    "                # freeze base model's layers\n",
    "                param.requires_grad = False\n",
    "\n",
    "                # cast all non INT8 parameters to fp32\n",
    "            for param in self.model.parameters():\n",
    "                if (param.dtype == torch.float16) or (param.dtype == torch.bfloat16):\n",
    "                    param.data = param.data.to(torch.float32)\n",
    "\n",
    "            # Form peft-model\n",
    "            lora_r = 16\n",
    "            lora_alpha = 16\n",
    "            lora_target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'lm_head']\n",
    "            lora_dropout = 0.05\n",
    "\n",
    "            LORA_config = LoraConfig(r=lora_r, lora_alpha=lora_alpha, target_modules=lora_target_modules,\n",
    "                                     lora_dropout=lora_dropout, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "            self.model = get_peft_model(self.model, LORA_config)\n",
    "\n",
    "            if self.verbose:\n",
    "                print()\n",
    "                self.model.print_trainable_parameters()\n",
    "                print()\n",
    "            dist.barrier()\n",
    "            self.model = self.model.to(args.gpu)\n",
    "\n",
    "        # Set up first_model\n",
    "        if train:\n",
    "            self.first_model = FP(llama_embed=self.train_loader.dataset.llama_embed.to(args.gpu),\n",
    "                                  real=self.train_loader.dataset.real_feature.to(args.gpu))\n",
    "        else:\n",
    "            self.first_model = FP(llama_embed=self.val_loader.dataset.llama_embed.to(args.gpu),\n",
    "                                  real=self.val_loader.dataset.real_feature.to(args.gpu))\n",
    "\n",
    "        re_start = 0  # first_model restart way contral\n",
    "        if train and re_start == 1:\n",
    "            print('All processes re-starting first-model')\n",
    "            ckpt_path = \"yours.pth\"\n",
    "\n",
    "            self.load_checkpoint(ckpt_path)\n",
    "            if self.verbose:\n",
    "                print('first_model loaded.')\n",
    "        if train and re_start == 2:\n",
    "            self.first_model.sln.bias.data = load_pickle('./first_restart/sln_bias.pkl')\n",
    "            self.first_model.sln.weight.data = load_pickle('./first_restart/sln_weight.pkl')\n",
    "            self.first_model.trans_1.weight.data = load_pickle('./first_restart/trans_1_weight.pkl')\n",
    "            self.first_model.trans_2.weight.data = load_pickle('./first_restart/trans_2_weight.pkl')\n",
    "            print('first model loaded')\n",
    "\n",
    "        self.first_model = self.first_model.to(args.gpu)\n",
    "\n",
    "        # GPU Options\n",
    "        print(f'Model Launching at GPU {self.args.gpu}')\n",
    "        if self.verbose:\n",
    "            start = time()\n",
    "\n",
    "        if args.multiGPU and not args.inference:\n",
    "            self.optim, self.lr_scheduler = self.create_optimizer_and_scheduler()\n",
    "\n",
    "            if args.distributed:  # DDP setup\n",
    "                self.model = DDP(self.model, device_ids=[args.gpu])\n",
    "                self.first_model = DDP(self.first_model, device_ids=[args.gpu])\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'It took {time() - start:.1f}s')\n",
    "\n",
    "        self.val_list = val_list\n",
    "\n",
    "    def train(self):\n",
    "        LOSSES_NAME = self.args.LOSSES_NAME\n",
    "\n",
    "        if self.verbose:\n",
    "            loss_meters = [LossMeter() for _ in range(len(LOSSES_NAME))]\n",
    "            best_eval_loss = 100000.\n",
    "\n",
    "            project_name = \"Natural Language is All a Graph Needs\"\n",
    "\n",
    "            src_dir = Path(__file__).resolve().parent\n",
    "            base_path = str(src_dir.parent)\n",
    "            src_dir = str(src_dir)\n",
    "\n",
    "        if self.args.distributed:\n",
    "            dist.barrier()\n",
    "\n",
    "        for epoch in range(self.args.epoch):\n",
    "            global_step = 0\n",
    "\n",
    "            if self.args.distributed:\n",
    "                self.train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "                # Train\n",
    "            self.model.train()\n",
    "            self.first_model.train()\n",
    "\n",
    "            if self.verbose:\n",
    "                pbar = tqdm(total=len(self.train_loader), ncols=275)\n",
    "\n",
    "            epoch_results = {}\n",
    "            for loss_name in LOSSES_NAME:\n",
    "                epoch_results[loss_name] = 0.\n",
    "                epoch_results[f'{loss_name}_count'] = 0\n",
    "\n",
    "            for step_i, batch in enumerate(self.train_loader):\n",
    "                torch.cuda.empty_cache()\n",
    "                dist.barrier()\n",
    "\n",
    "                if self.args.fp16 and _use_native_amp:\n",
    "                    pass\n",
    "                else:\n",
    "                    if self.args.distributed:\n",
    "\n",
    "                        dddd = next(self.model.parameters()).device\n",
    "\n",
    "                        input_ids = batch['input_ids'].to(dddd)\n",
    "                        lm_labels = batch[\"target_ids\"].to(dddd)\n",
    "                        attention_mask = batch['attn_mask'].to(dddd)\n",
    "\n",
    "                        loss_weights = batch[\"loss_weights\"].to(dddd)\n",
    "                        B, L = lm_labels.size()\n",
    "\n",
    "                        embeds = self.first_model(  # forward\n",
    "                            input_ids=input_ids\n",
    "                        )\n",
    "                        # forward\n",
    "                        # Notably, our input for self.mode here is numberical inputs_embeds, \n",
    "                        # i.e. we already map inputs_ids to embeddings in via self.first_model\n",
    "                        output = self.model(inputs_embeds=embeds, attention_mask=attention_mask, labels=lm_labels)\n",
    "\n",
    "                        lm_mask = lm_labels[:, 1:] != -100\n",
    "                        lm_mask = lm_mask.float()\n",
    "\n",
    "                        loss = output['loss']\n",
    "\n",
    "                        loss = loss.view(B, L - 1) * lm_mask\n",
    "\n",
    "                        loss = loss.sum(dim=1) / lm_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "                        results = {}\n",
    "\n",
    "                        results['loss'] = (loss * loss_weights).mean()\n",
    "                        results['total_loss'] = loss.detach().sum()\n",
    "                        results['total_loss_count'] = len(loss)\n",
    "\n",
    "                        task_counts = {task: 0 for task in self.model.module.losses}\n",
    "                        task_loss = {task: 0 for task in self.model.module.losses}\n",
    "\n",
    "                        for _loss, task in zip(loss.detach(), batch['task']):\n",
    "                            task_loss[task] += _loss\n",
    "                            task_counts[task] += 1\n",
    "\n",
    "                        for task in self.model.module.losses:\n",
    "                            if task_counts[task] > 0:\n",
    "                                results[f'{task}_loss'] = task_loss[task]\n",
    "                                results[f'{task}_loss_count'] = task_counts[task]\n",
    "\n",
    "                    else:\n",
    "                        results = self.model.train_step(batch)\n",
    "\n",
    "                loss = results['loss'] / self.args.gradient_accumulation_steps\n",
    "                dist.barrier()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                loss = loss.detach()\n",
    "\n",
    "                # Update Parameters\n",
    "                if step_i % self.args.gradient_accumulation_steps == 0:\n",
    "\n",
    "                    if self.args.clip_grad_norm > 0:\n",
    "                        if self.args.fp16 and _use_native_amp:\n",
    "                            self.scaler.unscale_(self.optim)\n",
    "                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.clip_grad_norm)\n",
    "                        elif self.args.fp16 and _use_apex:\n",
    "                            torch.nn.utils.clip_grad_norm_(amp.master_params(self.optim), self.args.clip_grad_norm)\n",
    "                        else:\n",
    "                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.clip_grad_norm)\n",
    "                            torch.nn.utils.clip_grad_norm_(self.first_model.parameters(), self.args.clip_grad_norm)\n",
    "\n",
    "                    if self.args.fp16 and _use_native_amp:\n",
    "                        self.scaler.step(self.optim)\n",
    "                        self.scaler.update()\n",
    "                    else:\n",
    "                        self.optim.step()  # update\n",
    "\n",
    "                    if self.lr_scheduler:\n",
    "                        self.lr_scheduler.step()\n",
    "\n",
    "                    for param in self.model.parameters():\n",
    "                        param.grad = None\n",
    "                    for param in self.first_model.parameters():\n",
    "                        param.grad = None\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                # 32 attention block layers in 7b \n",
    "                # Another way for saving checkpoint is 'self.model.save_pretrained(\"checkpoint_fold_name\")'\n",
    "                if global_step == len(self.train_loader) // 8:\n",
    "                    if self.verbose:\n",
    "                        for ig in range(32):\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid1/Gqa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid1/Gka_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid1/Gva_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid1/Goa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid1/Gqb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid1/Gkb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid1/Gvb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid1/Gob_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_A.default.weight,\n",
    "                                    \"./llama_{}_mmid1/Glm_a_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_B.default.weight,\n",
    "                                    \"./llama_{}_mmid1/Glm_b_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "\n",
    "                        torch.save(self.first_model.state_dict(),\n",
    "                                   \"Gfirst_{}_{}_8_{}_mmid1.pth\".format(epoch + 1, self.args.lr, self.args.train))\n",
    "                if global_step == len(self.train_loader) // 4:\n",
    "                    if self.verbose:\n",
    "                        for ig in range(32):\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid1/Gqa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid1/Gka_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid1/Gva_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid1/Goa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid1/Gqb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid1/Gkb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid1/Gvb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid1/Gob_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_A.default.weight,\n",
    "                                    \"./llama_{}_mid1/Glm_a_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_B.default.weight,\n",
    "                                    \"./llama_{}_mid1/Glm_b_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "\n",
    "                        torch.save(self.first_model.state_dict(),\n",
    "                                   \"Gfirst_{}_{}_8_{}_mid1.pth\".format(epoch + 1, self.args.lr, self.args.train))\n",
    "                if global_step == len(self.train_loader) * 3 // 8:\n",
    "                    if self.verbose:\n",
    "                        for ig in range(32):\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid2/Gqa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid2/Gka_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid2/Gva_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid2/Goa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid2/Gqb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid2/Gkb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid2/Gvb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid2/Gob_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_A.default.weight,\n",
    "                                    \"./llama_{}_mmid2/Glm_a_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_B.default.weight,\n",
    "                                    \"./llama_{}_mmid2/Glm_b_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "\n",
    "                        torch.save(self.first_model.state_dict(),\n",
    "                                   \"Gfirst_{}_{}_8_{}_mmid2.pth\".format(epoch + 1, self.args.lr, self.args.train))\n",
    "                if global_step == len(self.train_loader) // 2:\n",
    "                    if self.verbose:\n",
    "                        for ig in range(32):\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid2/Gqa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid2/Gka_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid2/Gva_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid2/Goa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid2/Gqb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid2/Gkb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid2/Gvb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid2/Gob_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_A.default.weight,\n",
    "                                    \"./llama_{}_mid2/Glm_a_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_B.default.weight,\n",
    "                                    \"./llama_{}_mid2/Glm_b_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "\n",
    "                        torch.save(self.first_model.state_dict(),\n",
    "                                   \"Gfirst_{}_{}_8_{}_mid2.pth\".format(epoch + 1, self.args.lr, self.args.train))\n",
    "                if global_step == len(self.train_loader) * 5 // 8:\n",
    "                    if self.verbose:\n",
    "                        for ig in range(32):\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid3/Gqa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid3/Gka_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid3/Gva_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mmid3/Goa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid3/Gqb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid3/Gkb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid3/Gvb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mmid3/Gob_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_A.default.weight,\n",
    "                                    \"./llama_{}_mmid3/Glm_a_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_B.default.weight,\n",
    "                                    \"./llama_{}_mmid3/Glm_b_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "\n",
    "                        torch.save(self.first_model.state_dict(),\n",
    "                                   \"Gfirst_{}_{}_8_{}_mmid3.pth\".format(epoch + 1, self.args.lr, self.args.train))\n",
    "                if global_step == len(self.train_loader) * 3 // 4:\n",
    "                    if self.verbose:\n",
    "                        for ig in range(32):\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid3/Gqa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid3/Gka_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid3/Gva_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mid3/Goa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid3/Gqb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid3/Gkb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid3/Gvb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mid3/Gob_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_A.default.weight,\n",
    "                                    \"./llama_{}_mid3/Glm_a_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_B.default.weight,\n",
    "                                    \"./llama_{}_mid3/Glm_b_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "\n",
    "                        torch.save(self.first_model.state_dict(),\n",
    "                                   \"Gfirst_{}_{}_8_{}_mid3.pth\".format(epoch + 1, self.args.lr, self.args.train))\n",
    "                if global_step == len(self.train_loader) * 7 // 8:\n",
    "                    if self.verbose:\n",
    "                        for ig in range(32):\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mend/Gqa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mend/Gka_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mend/Gva_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_A.default.weight.data,\n",
    "                                        \"./llama_{}_mend/Goa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.q_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mend/Gqb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.k_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mend/Gkb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.v_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mend/Gvb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                            save_pickle(self.model.module.base_model.model.model.layers[\n",
    "                                            ig].self_attn.o_proj.lora_B.default.weight.data,\n",
    "                                        \"./llama_{}_mend/Gob_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_A.default.weight,\n",
    "                                    \"./llama_{}_mend/Glm_a_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "                        save_pickle(self.model.module.base_model.model.lm_head.lora_B.default.weight,\n",
    "                                    \"./llama_{}_mend/Glm_b_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "\n",
    "                        torch.save(self.first_model.state_dict(),\n",
    "                                   \"Gfirst_{}_{}_8_{}_mend.pth\".format(epoch + 1, self.args.lr, self.args.train))\n",
    "\n",
    "                dist.barrier()\n",
    "\n",
    "                if self.lr_scheduler:\n",
    "                    if version.parse(torch.__version__) >= version.parse(\"1.4\"):\n",
    "                        lr = self.lr_scheduler.get_last_lr()[0]\n",
    "                    else:\n",
    "                        lr = self.lr_scheduler.get_lr()[0]\n",
    "                else:\n",
    "                    lr = self.optim.param_groups[-1]['lr']\n",
    "\n",
    "                for k, v in results.items():\n",
    "                    if k in epoch_results:\n",
    "                        if isinstance(v, int):\n",
    "                            epoch_results[k] += v\n",
    "                        elif isinstance(v, torch.Tensor):\n",
    "                            epoch_results[k] += v.item()\n",
    "\n",
    "                if self.verbose and step_i % 1 == 0:\n",
    "                    desc_str = f'Epoch {epoch} | LR {lr:.6f} |'\n",
    "\n",
    "                    for i, (loss_name, loss_meter) in enumerate(zip(LOSSES_NAME, loss_meters)):\n",
    "\n",
    "                        if loss_name in results:\n",
    "                            loss_meter.update(results[f'{loss_name}'] / results[f'{loss_name}_count'])\n",
    "                        if len(loss_meter) > 0:\n",
    "                            loss_count = epoch_results[f'{loss_name}_count']\n",
    "                            desc_str += f' {loss_name} ({loss_count}) {loss_meter.val:.3f}'\n",
    "\n",
    "                    pbar.set_description(desc_str)\n",
    "                    pbar.update(1)\n",
    "                dist.barrier()\n",
    "\n",
    "            if self.verbose:\n",
    "                pbar.close()\n",
    "\n",
    "            dist.barrier()\n",
    "\n",
    "            results = reduce_dict(epoch_results, average=False)  # For global information\n",
    "\n",
    "            dist.barrier()\n",
    "\n",
    "            if self.verbose:\n",
    "                train_loss = results['total_loss']\n",
    "                train_loss_count = results['total_loss_count']\n",
    "\n",
    "                avg_train_loss = train_loss / train_loss_count\n",
    "                losses_str = f\"Train Loss: {avg_train_loss:.3f}\\n\"\n",
    "\n",
    "                for name, loss in results.items():\n",
    "                    if name[-4:] == 'loss':\n",
    "                        loss_count = int(results[name + '_count'])\n",
    "                        if loss_count > 0:\n",
    "                            avg_loss = loss / loss_count\n",
    "                            losses_str += f\"{name} ({loss_count}): {avg_loss:.3f} \"\n",
    "\n",
    "                losses_str += '\\n'\n",
    "                print(losses_str)  # print once per epoch\n",
    "\n",
    "            dist.barrier()\n",
    "\n",
    "            if self.verbose:\n",
    "                for ig in range(32):\n",
    "                    save_pickle(\n",
    "                        self.model.module.base_model.model.model.layers[ig].self_attn.q_proj.lora_A.default.weight.data,\n",
    "                        \"./llama_{}_end/Gqa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                    save_pickle(\n",
    "                        self.model.module.base_model.model.model.layers[ig].self_attn.k_proj.lora_A.default.weight.data,\n",
    "                        \"./llama_{}_end/Gka_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                    save_pickle(\n",
    "                        self.model.module.base_model.model.model.layers[ig].self_attn.v_proj.lora_A.default.weight.data,\n",
    "                        \"./llama_{}_end/Gva_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                    save_pickle(\n",
    "                        self.model.module.base_model.model.model.layers[ig].self_attn.o_proj.lora_A.default.weight.data,\n",
    "                        \"./llama_{}_end/Goa_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                    save_pickle(\n",
    "                        self.model.module.base_model.model.model.layers[ig].self_attn.q_proj.lora_B.default.weight.data,\n",
    "                        \"./llama_{}_end/Gqb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                    save_pickle(\n",
    "                        self.model.module.base_model.model.model.layers[ig].self_attn.k_proj.lora_B.default.weight.data,\n",
    "                        \"./llama_{}_end/Gkb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                    save_pickle(\n",
    "                        self.model.module.base_model.model.model.layers[ig].self_attn.v_proj.lora_B.default.weight.data,\n",
    "                        \"./llama_{}_end/Gvb_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                    save_pickle(\n",
    "                        self.model.module.base_model.model.model.layers[ig].self_attn.o_proj.lora_B.default.weight.data,\n",
    "                        \"./llama_{}_end/Gob_{}_{}.pkl\".format(epoch + 1, ig, self.args.lr))\n",
    "                save_pickle(self.model.module.base_model.model.lm_head.lora_A.default.weight,\n",
    "                            \"./llama_{}_end/Glm_a_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "                save_pickle(self.model.module.base_model.model.lm_head.lora_B.default.weight,\n",
    "                            \"./llama_{}_end/Glm_b_{}.pkl\".format(epoch + 1, self.args.lr))\n",
    "\n",
    "                torch.save(self.first_model.state_dict(),\n",
    "                           \"Gfirst_{}_{}_8_{}_end.pth\".format(epoch + 1, self.args.lr, self.args.train))\n",
    "            dist.barrier()\n",
    "\n",
    "    def test(self):\n",
    "        for epoch in range(8 * self.args.epoch):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if (epoch + 1) % 8 == 1:\n",
    "                doc_prefix = './llama_{}_mmid1/'.format(epoch // 8 + 1)\n",
    "                ckpt_path = \"Gfirst_{}_{}_8_{}_mmid1.pth\".format(epoch // 8 + 1, self.args.lr, self.args.train)\n",
    "            elif (epoch + 1) % 8 == 2:\n",
    "                doc_prefix = './llama_{}_mid1/'.format(epoch // 8 + 1)\n",
    "                ckpt_path = \"Gfirst_{}_{}_8_{}_mid1.pth\".format(epoch // 8 + 1, self.args.lr, self.args.train)\n",
    "            elif (epoch + 1) % 8 == 3:\n",
    "                doc_prefix = './llama_{}_mmid2/'.format(epoch // 8 + 1)\n",
    "                ckpt_path = \"Gfirst_{}_{}_8_{}_mmid2.pth\".format(epoch // 8 + 1, self.args.lr, self.args.train)\n",
    "            elif (epoch + 1) % 8 == 4:\n",
    "                doc_prefix = './llama_{}_mid2/'.format(epoch // 8 + 1)\n",
    "                ckpt_path = \"Gfirst_{}_{}_8_{}_mid2.pth\".format(epoch // 8 + 1, self.args.lr, self.args.train)\n",
    "            elif (epoch + 1) % 8 == 5:\n",
    "                doc_prefix = './llama_{}_mmid3/'.format(epoch // 8 + 1)\n",
    "                ckpt_path = \"Gfirst_{}_{}_8_{}_mmid3.pth\".format(epoch // 8 + 1, self.args.lr, self.args.train)\n",
    "            elif (epoch + 1) % 8 == 6:\n",
    "                doc_prefix = './llama_{}_mid3/'.format(epoch // 8 + 1)\n",
    "                ckpt_path = \"Gfirst_{}_{}_8_{}_mid3.pth\".format(epoch // 8 + 1, self.args.lr, self.args.train)\n",
    "            elif (epoch + 1) % 8 == 7:\n",
    "                doc_prefix = './llama_{}_mend/'.format(epoch // 8 + 1)\n",
    "                ckpt_path = \"Gfirst_{}_{}_8_{}_mend.pth\".format(epoch // 8 + 1, self.args.lr, self.args.train)\n",
    "            else:\n",
    "                doc_prefix = './llama_{}_end/'.format(epoch // 8 + 1)\n",
    "                ckpt_path = \"Gfirst_{}_{}_8_{}_end.pth\".format(epoch // 8 + 1, self.args.lr, self.args.train)\n",
    "\n",
    "            # One can directly assign the checkpoint here when testing.\n",
    "            ##doc_prefix='./your_main_model_pickle_fold/'\n",
    "            ##ckpt_path='your_first_model.pth'\n",
    "\n",
    "            self.load_checkpoint(ckpt_path)\n",
    "            self.first_model = self.first_model.to(self.args.gpu)\n",
    "\n",
    "            for gg in range(32):\n",
    "                self.model.base_model.model.model.layers[gg].self_attn.q_proj.lora_A.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Gqa_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                self.model.base_model.model.model.layers[gg].self_attn.k_proj.lora_A.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Gka_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                self.model.base_model.model.model.layers[gg].self_attn.v_proj.lora_A.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Gva_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                self.model.base_model.model.model.layers[gg].self_attn.o_proj.lora_A.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Goa_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                self.model.base_model.model.model.layers[gg].self_attn.q_proj.lora_B.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Gqb_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                self.model.base_model.model.model.layers[gg].self_attn.k_proj.lora_B.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Gkb_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                self.model.base_model.model.model.layers[gg].self_attn.v_proj.lora_B.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Gvb_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "                self.model.base_model.model.model.layers[gg].self_attn.o_proj.lora_B.default.weight.data = load_pickle(\n",
    "                    doc_prefix + \"Gob_{}_{}.pkl\".format(gg, self.args.lr)).to(args.gpu)\n",
    "            self.model.base_model.model.lm_head.lora_A.default.weight.data = load_pickle(\n",
    "                doc_prefix + \"Glm_a_{}.pkl\".format(self.args.lr)).data.to(args.gpu)\n",
    "            self.model.base_model.model.lm_head.lora_B.default.weight.data = load_pickle(\n",
    "                doc_prefix + \"Glm_b_{}.pkl\".format(self.args.lr)).data.to(args.gpu)\n",
    "            if self.verbose:\n",
    "                print('Main model loaded.')\n",
    "            self.model = self.model.to(self.args.gpu)\n",
    "\n",
    "            valid_results = self.evaluate_epoch()  # For accuracy\n",
    "            dist.barrier()\n",
    "\n",
    "            valid_results = new_reduce_dict(valid_results)\n",
    "            dist.barrier()\n",
    "\n",
    "            if self.verbose:\n",
    "                print()\n",
    "                print()\n",
    "                for kk in valid_results.keys():\n",
    "                    if kk.endswith('transductive'):\n",
    "                        if self.args.train == 'Arxiv':\n",
    "                            valid_results[kk] = valid_results[kk].item() / self.val_loader.dataset.len_transductive\n",
    "                print(valid_results)\n",
    "                print()\n",
    "                print()\n",
    "\n",
    "            dist.barrier()\n",
    "\n",
    "            if self.verbose:\n",
    "                acc_file = open('Llama_7b.txt', 'a')\n",
    "                if (epoch + 1) % 8 == 1:\n",
    "                    acc_file.write(str(epoch // 8 + 1) + '_mmid1' + '\\n')\n",
    "                elif (epoch + 1) % 8 == 2:\n",
    "                    acc_file.write(str(epoch // 8 + 1) + '_mid1' + '\\n')\n",
    "                elif (epoch + 1) % 8 == 3:\n",
    "                    acc_file.write(str(epoch // 8 + 1) + '_mmid2' + '\\n')\n",
    "                elif (epoch + 1) % 8 == 4:\n",
    "                    acc_file.write(str(epoch // 8 + 1) + '_mid2' + '\\n')\n",
    "                elif (epoch + 1) % 8 == 5:\n",
    "                    acc_file.write(str(epoch // 8 + 1) + '_mmid3' + '\\n')\n",
    "                elif (epoch + 1) % 8 == 6:\n",
    "                    acc_file.write(str(epoch // 8 + 1) + '_mid3' + '\\n')\n",
    "                elif (epoch + 1) % 8 == 7:\n",
    "                    acc_file.write(str(epoch // 8 + 1) + '_mend' + '\\n')\n",
    "                else:\n",
    "                    acc_file.write(str(epoch // 8 + 1) + '_end' + '\\n')\n",
    "                acc_file.write(str(valid_results) + '\\n\\n')\n",
    "                acc_file.close()\n",
    "            dist.barrier()\n",
    "\n",
    "    def evaluate_epoch(self):\n",
    "        ACC = {}\n",
    "        for k in list(self.val_list.keys()):\n",
    "            if k == 'link':\n",
    "                pass\n",
    "            elif k == 'classification':\n",
    "                if self.args.train == 'Arxiv':\n",
    "                    templates = []\n",
    "                    for tems in self.val_list[k]:\n",
    "                        templates = templates + tems\n",
    "                    for thing in templates:\n",
    "                        ACC[thing + '-' + 'transductive'] = 0\n",
    "        self.model.eval()\n",
    "        self.first_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step_i, batch in tqdm(enumerate(self.val_loader)):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                if self.args.distributed:\n",
    "                    device = next(self.model.parameters()).device\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    embeds = self.first_model(input_ids=input_ids)\n",
    "                    attention_mask = batch['attn_mask'].to(device)\n",
    "                    results = self.model.g_step(in_embeds=embeds, attention_mask=attention_mask)\n",
    "\n",
    "                for iiid in range(len(results)):\n",
    "                    task = batch['task'][iiid]\n",
    "                    temp_id = batch['temp_ids'][iiid]\n",
    "                    if task == 'classification':\n",
    "                        cate = batch['cate'][iiid]\n",
    "                        if temp_id.endswith('2') or temp_id.endswith('4') or temp_id.endswith('6') or temp_id.endswith(\n",
    "                                '7'):\n",
    "                            if results[iiid].lower() == batch['target_text'][iiid]:\n",
    "                                ACC[temp_id + '-' + cate] += 1\n",
    "                                # Check if the generated text strings is strictly matched with the label in natural language.\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                    elif task == 'link':\n",
    "                        pass\n",
    "\n",
    "                dist.barrier()\n",
    "\n",
    "            return ACC\n"
   ],
   "id": "d38e4ca0bb7edb4b",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-02T06:45:42.833330Z"
    }
   },
   "cell_type": "code",
   "source": "trainer = Trainer(args=args, train_loader=train_loader, train=True)",
   "id": "7da8301abdf14299",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model at GPU mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "267259bb8dc944bfb4f84768e8431f55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
