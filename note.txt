# 此文件包含一些关键注释和实现。

1.  我们使用一个简单的MLP来执行节点嵌入的维度转换。
    例如，我们将GIANT（768）映射到Llama-7b嵌入大小（4096）。
    这样的MLP也可以被视为将嵌入LLM的原始数字节点转换到LLM的语义空间中的语义适配器。

2.  对于标记化，值得注意的是，我们没有扩展标记化器。
    即，我们不为令牌化器添加新的节点令牌，这样的实现是不可扩展的，因为它会严重减慢令牌化过程。
    相反，我们在标记化过程中使用固定的特殊标记“＜extra_id_0＞”作为占位符。
    “＜extra_id_0＞”将占据自然语言文本中节点标记的所有位置，同时相应地记录真实的节点标记id。
    然后，编码的input_ids将根据真实的节点ID进行正确的修改，这样的工程操作可以大大加快管道的速度，尤其是当图很大的时候。
    细节可以在arxiv.py中找到，其中形成了图结构描述语句。

3.  在指令调整方面：
    对于Arxiv和Pubmed，我们通过首先将每个历元的节点示例增加一倍并使用lr 8e-5对2个历元执行无图调整来获得最佳结果，
    然后我们通过验证选择最好的一个，并进一步添加自监督提示和多跳结构描述提示，用于lr3e-5的另外两个历元训练。
    然而，对于Cora，我们发现从训练一开始，无融合图提示/多跳结构提示/链接预测提示就会收敛到最佳结果。

4. 对于Flan-t5系列模型（微调），我们扩展了输入嵌入词汇表和最终的lm_head，因此Flan-t5支持生成和判别链接预测提示。
   对于Llama-7b（LoRA），我们只扩展了输入嵌入词汇表（即first_model），因此Llama-7b只支持判别链接预测提示。

5. 对于Llama中的检查点保存和加载，我们提供了两种不同的方式和相应的检查点。
   如果单个GPU内存小于25G，我们在现有文件中显式编码的方式可以帮助避免CUDA内存不足。

6. 我们提供 *.sh 文件在./scripts目录以用来 training/inference
   除了*.sh文件中的可调参数之外，
   通过pretrain.py中main_worker函数中的控制台选择用于训练/val/test的提示也很方便
   并控制单个epoch的示例数量&arxiv.py中不同类型提示之间的示例分布比率（按任务类型和跳级分组）。